# -*- coding: utf-8 -*-
"""AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TTNBFlI-U5SEXzWUAzWlC0Yn7hXrTuRO
"""

!apt update
!apt install nvidia-driver-535

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

pip install opencv-python face_recognition numpy pandas openpyxl

from google.colab import files

# Create a dataset directory
!mkdir -p dataset

# Upload dataset files
uploaded = files.upload()

# Move uploaded files to the dataset directory
for filename in uploaded.keys():
    !mv "{filename}" "dataset/{filename}"

print("Dataset files uploaded successfully!")

import cv2
import face_recognition
import os
import numpy as np
import pandas as pd
from datetime import datetime
from google.colab.patches import cv2_imshow
from IPython.display import display, Javascript
from google.colab import output
import base64

def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
        async function takePhoto(quality) {
          try {
            const div = document.createElement('div');
            const capture = document.createElement('button');
            capture.textContent = 'Capture';
            div.appendChild(capture);

            const video = document.createElement('video');
            video.style.display = 'block';
            // Request camera access with permissions
            const stream = await navigator.mediaDevices.getUserMedia({video: { facingMode: "environment" }});

            document.body.appendChild(div);
            div.appendChild(video);
            video.srcObject = stream;
            await video.play();

            // Resize the output to fit the video element.
            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

            // Wait for Capture to be clicked.
            await new Promise((resolve) => capture.onclick = resolve);

            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            canvas.getContext('2d').drawImage(video, 0, 0);
            stream.getVideoTracks()[0].stop();
            div.remove();
            return canvas.toDataURL('image/jpeg', quality);
          } catch (error) {
            console.error('Error accessing camera:', error);
            // Return an error message to Python
            return 'Error: ' + error.message;
          }
        }
        ''')
    display(js)
    data = output.eval_js('takePhoto({})'.format(quality))
    # Check for error message from JavaScript
    if data.startswith('Error: '):
        raise RuntimeError(data)
    binary = base64.b64decode(data.split(',')[1])
    with open(filename, 'wb') as f:
        f.write(binary)
    return filename


def load_known_faces(dataset_path):
    known_faces = []
    known_names = []
    for filename in os.listdir(dataset_path):
        if filename.endswith(('.jpg', '.png')):
            image = face_recognition.load_image_file(os.path.join(dataset_path, filename))
            encoding = face_recognition.face_encodings(image)[0]
            known_faces.append(encoding)
            known_names.append(os.path.splitext(filename)[0])
    return known_faces, known_names

def mark_attendance(name, sap_id):
    now = datetime.now()
    date = now.strftime("%Y-%m-%d")
    time = now.strftime("%H:%M:%S")

    # Check if the file exists and if it's empty
    if os.path.exists('attendance.xlsx'):
        df = pd.read_excel('attendance.xlsx')
        # Check if the DataFrame is empty, if so, add columns
        if df.empty:
            df = pd.DataFrame(columns=['Name', 'SAP ID', 'Date', 'Time'])
    else:
        df = pd.DataFrame(columns=['Name', 'SAP ID', 'Date', 'Time'])

    if not ((df['Name'] == name) & (df['Date'] == date)).any():
        new_row = {'Name': name, 'SAP ID': sap_id, 'Date': date, 'Time': time}
        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
        df.to_excel('attendance.xlsx', index=False)
        print(f"Marked attendance for {name}")
    else:
        print(f"{name} already marked present today")


def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});

          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          // Resize the output to fit the video element.
          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          // Wait for Capture to be clicked.
          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getVideoTracks()[0].stop();
          div.remove();
          return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
    display(js)
    data = output.eval_js('takePhoto({})'.format(quality))
    binary = base64.b64decode(data.split(',')[1])
    with open(filename, 'wb') as f:
        f.write(binary)
    return filename

def main():
    dataset_path = 'dataset'
    known_faces, known_names = load_known_faces(dataset_path)

    while True:
        print("Press Enter to capture an image (or type 'q' to quit):")
        if input() == 'q':
            break

        filename = take_photo()
        frame = cv2.imread(filename)

        face_locations = face_recognition.face_locations(frame)
        face_encodings = face_recognition.face_encodings(frame, face_locations)

        for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):
            matches = face_recognition.compare_faces(known_faces, face_encoding)
            name = "Unknown"
            sap_id = "Unknown"

            if True in matches:
                first_match_index = matches.index(True)
                full_name = known_names[first_match_index]

                # Check if the filename contains an underscore before splitting
                if '_' in full_name:
                    name, sap_id = full_name.split('_')
                else:
                    print(f"Warning: Filename '{full_name}' does not contain an underscore. Skipping...")
                    # Handle the case where the filename doesn't have an underscore
                    # For example, you could use the filename as the name and set sap_id to "Unknown"
                    # name = full_name
                    # sap_id = "Unknown"

                mark_attendance(name, sap_id)

            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
            cv2.putText(frame, f"{name} ({sap_id})", (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)

        cv2_imshow(frame)

    print("Attendance marking complete. Check the 'attendance.xlsx' file for records.")

if __name__ == "_main_":
    main()

main()

from google.colab import files
files.download('attendance.xlsx')

import pandas as pd

# Data for the table
data = {
    "Aspect": [
        "Multimodal System",
        "Type",
        "Fusion Level",
        "Accuracy",
        "Application",
        "ML Used",
        "Remarks for Improvement"
    ],
    "Paper 1: For Your Eyes Only [7†source]": [
        "Focuses on retina-based biometrics for secure authentication.",
        "Unimodal system (retina only).",
        "NA – Retinal recognition does not rely on fusion.",
        "Very high due to unique and stable retinal patterns.",
        "High-security environments: banking, government, and privacy-critical systems.",
        "Homomorphic Encryption for template protection.",
        "Integrate cost-effective scanning devices to reduce system cost and complexity."
    ],
    "Paper 2: Multimodal Biometrics [8†source]": [
        "Combines physiological (fingerprint, iris) and behavioral (voice, gait) traits.",
        "Multimodal system with multiple biometric traits.",
        "Supports fusion at feature, match score, and decision levels.",
        "Improved compared to unimodal systems due to integration of multiple traits.",
        "Defense, law enforcement, enterprise fraud prevention.",
        "Uses machine learning for feature fusion and classification.",
        "Enhance sensor integration and develop standards for interoperability."
    ],
    "Paper 3: A Study on Multimodal Biometrics System [9†source]": [
        "Integrates multiple biometric modalities for robust authentication.",
        "Multimodal, combining various physiological and behavioral traits.",
        "Feature-level fusion preferred for accuracy; match-score fusion commonly used.",
        "Consistently outperforms unimodal systems in FAR and FRR metrics.",
        "Border control, healthcare, financial fraud prevention, and surveillance.",
        "SVM, Decision Trees, and Neural Networks for classification.",
        "Focus on real-time scalability and robust fusion mechanisms."
    ],
    "Paper 4: Module-wise Adaptive Distillation [8†source]": [
        "Supports fusion of modalities at module level.",
        "Modular and multimodal system for improved adaptability.",
        "Applies module-wise fusion for optimal results.",
        "Enhances multimodal accuracy through adaptive distillation techniques.",
        "Multimodal applications requiring adaptive learning.",
        "Adaptive Distillation for modular multimodality learning.",
        "Expand applicability across diverse datasets and tasks."
    ],
    "Paper 5: Open Visual Knowledge Extraction [9†source]": [
        "Uses multimodal relations to extract and link visual knowledge.",
        "Multimodal system focused on knowledge extraction.",
        "Fusion of visual and textual prompts for relation-based knowledge generation.",
        "Achieves robust accuracy in extracting knowledge across modalities.",
        "Knowledge extraction for AI systems and visual reasoning tasks.",
        "Neural prompts for multimodal relation extraction.",
        "Explore more generalized prompts for broader knowledge extraction."
    ]
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Save as Excel file
excel_file_path = 'Comparison_of_Biometrics_Papers.xlsx'
df.to_excel(excel_file_path, index=False)

# Save as CSV file
csv_file_path = 'Comparison_of_Biometrics_Papers.csv'
df.to_csv(csv_file_path, index=False)

print(f"Excel file saved as: {excel_file_path}")
print(f"CSV file saved as: {csv_file_path}")

import face_recognition
import cv2
import os

# Paths to your dataset and test set
dataset_path = 'NewDataset'
testset_path = 'NewTestset'

# Load known faces from your dataset
def load_known_faces(dataset_path):
    known_faces = []
    known_names = []
    for filename in os.listdir(dataset_path):
        if filename.endswith(('.jpg', '.png')):
            image = face_recognition.load_image_file(os.path.join(dataset_path, filename))
            encoding = face_recognition.face_encodings(image)[0]
            known_faces.append(encoding)
            known_names.append(os.path.splitext(filename)[0])  # Expecting "name_sapID" format
    return known_faces, known_names

# Initialize known faces and names
known_faces, known_names = load_known_faces(dataset_path)

# Define counters for accuracy metrics
TP = 0  # True Positives
TN = 0  # True Negatives
FP = 0  # False Positives
FN = 0  # False Negatives

# Loop through test images and check for accuracy
for filename in os.listdir(testset_path):
    if filename.endswith(('.jpg', '.png')):
        # Load test image and expected label
        test_image = face_recognition.load_image_file(os.path.join(testset_path, filename))
        expected_label = os.path.splitext(filename)[0]  # Expected format: "name_sapID" or "Unknown"

        # Find all faces and face encodings in the test image
        face_locations = face_recognition.face_locations(test_image)
        face_encodings = face_recognition.face_encodings(test_image, face_locations)

        match_found = False
        for face_encoding in face_encodings:
            matches = face_recognition.compare_faces(known_faces, face_encoding)
            name = "Unknown"

            # Check for match
            if True in matches:
                match_index = matches.index(True)
                name = known_names[match_index]
                match_found = True

            # Evaluate the match
            if name == expected_label:
                if name == "Unknown":
                    TN += 1  # Correctly identified as unknown
                else:
                    TP += 1  # Correctly identified as known
            else:
                if name == "Unknown":
                    FN += 1  # Known person not recognized
                else:
                    FP += 1  # Incorrectly recognized as someone else

# Calculate accuracy metrics
accuracy = (TP + TN) / (TP + TN + FP + FN) * 100
precision = TP / (TP + FP) * 100 if (TP + FP) > 0 else 0
recall = TP / (TP + FN) * 100 if (TP + FN) > 0 else 0
f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

# Print metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1_score)

